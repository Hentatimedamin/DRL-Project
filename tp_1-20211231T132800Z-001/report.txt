


----------environment------------
CartPole-v1
A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.
Observation:
        Type: Box(4)
        Num     Observation               Min                     Max
        0       Cart Position             -2.4                    2.4
        1       Cart Velocity             -Inf                    Inf
        2       Pole Angle                -0.209 rad (-12 deg)    0.209 rad (12 deg)
        3       Pole Angular Velocity     -Inf                    Inf
    Actions:
        Type: Discrete(2)
        Num   Action
        0     Push cart to the left
        1     Push cart to the right
        Note: The amount the velocity that is reduced or increased is not
        fixed; it depends on the angle the pole is pointing. This is because
        the center of gravity of the pole increases the amount of energy needed
        to move the cart underneath it
    Reward:
        Reward is 1 for every step taken, including the termination step
    Starting State:
        All observations are assigned a uniform random value in [-0.05..0.05]
    Episode Termination:
        Pole Angle is more than 12 degrees.
        Cart Position is more than 2.4 (center of the cart reaches the edge of
        the display).
        Episode length is greater than 200.
        Solved Requirements:
        Considered solved when the average return is greater than or equal to
        195.0 over 100 consecutive trials.
        
 --------------------------DQN Agent---------------------------
	**Model architecture:
		In our implementation, the main and target networks are quite simple 			consisting of 3 densely connected layers with Relu activation functions
	**Model hyperparameters:
	    state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            seed (int): Random seed
            fc1_units (int): Number of nodes in first hidden layer
            fc2_units (int): Number of nodes in second hidden layer
	**Agent hyperparameters:
		BUFFER_SIZE = int(1e5)  # replay buffer size
		BATCH_SIZE = 64         # minibatch size
		GAMMA = 0.99            # discount factor
		TAU = 1e-3              # for soft update of target parameters
		LR = 5e-4               # learning rate 
		UPDATE_EVERY = 4        # how often to update the network

the agent has to:
	*compute the action to choose for a given state
	*store its experiences in a memory buffer
	*train the DNN by sampling a batch of experiences from the memory buffer
In addition to that, we have to pay attention to the exploration-exploitation trade-off that I explained in my previous post. To recall it rapidly, the agent is more likely to explore the environment in the beginning by choosing random actions because he has no idea about how the environment works. Through time steps, the agent gets more and more knowledge, so he is more likely to exploit his knowledge rather than picking random actions. For that purpose, we will use the epsilon greedy algorithm.
--------------------------Duel DQN agent--------------------------------------
	**Model architecture:
		we want to split the state-dependent action advantages and the state-values into two separate streams. We also define the forward pass of the network with the forward mapping.
	**Model hyperparameters:
	    state_size (int): Dimension of each state
            action_size (int): Dimension of each action
            seed (int): Random seed
            fc1_units (int): Number of nodes in first hidden layer
            fc2_units (int): Number of nodes in second hidden layer
	**Agent architecture:
		Besides the soft-update function and the learn function ,nothing changes from the standard DQN architecture
------------- learning algorithms-----------
Sarsamax:Qlearning algorithm
--------------Result------------------
Since our environment is simple, we did not notice a significant difference in the test phase, but in the training phase, the duel_agent converges quickly compared to the dqn_ agent. 
